{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e34f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13de1ac6",
   "metadata": {},
   "source": [
    "#### PyTorch Dataset\n",
    "\n",
    "   A dataset that reads coco_like JSON or reads per-image files directly and returns image and target. Use torchvision transforms for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e89c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_saffron.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "class SaffronKeypointDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotation_json, transforms=None):\n",
    "        \"\"\"\n",
    "        annotation_json: path to the coco_like list saved earlier\n",
    "        \"\"\"\n",
    "        with open(annotation_json, \"r\") as f:\n",
    "            self.records = json.load(f)\n",
    "        self.images_dir = images_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]\n",
    "        img_path = os.path.join(self.images_dir, rec['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes = torch.as_tensor(rec['boxes'], dtype=torch.float32)\n",
    "        labels = torch.as_tensor(rec['labels'], dtype=torch.int64)\n",
    "\n",
    "        # keypoints expected shape (N, K, 3) -> K=1\n",
    "        kps = rec.get('keypoints', [])\n",
    "        if len(kps) == 0:\n",
    "            keypoints = torch.zeros((boxes.shape[0], 1, 3), dtype=torch.float32)\n",
    "        else:\n",
    "            # convert list [x,y,v] to (N,1,3)\n",
    "            kps_tensor = torch.tensor(kps, dtype=torch.float32)\n",
    "            keypoints = kps_tensor.view(-1, 1, 3)\n",
    "\n",
    "        image_id = torch.tensor([rec['image_id']])\n",
    "        area = torch.as_tensor(rec.get('area', [ (b[2]-b[0])*(b[3]-b[1]) for b in rec['boxes'] ]), dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(rec.get('iscrowd', [0]*len(rec['boxes'])), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"keypoints\": keypoints,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "dataset = SaffronKeypointDataset(\n",
    "    images_dir=\"Dataset_txt/image\",\n",
    "    annotation_json=\"annotations_coco_like.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af62c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21514669",
   "metadata": {},
   "source": [
    "#### Transforms (using torchvision utilities)\n",
    "\n",
    "    Keypoint models require transforms that also transform keypoints. Here's a simple set modeled after torchvision references/detection transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20393f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms.py\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            image = F.hflip(image)\n",
    "            w, h = image.shape[2], image.shape[1]\n",
    "            # flip boxes\n",
    "            boxes = target['boxes']\n",
    "            boxes = boxes.clone()\n",
    "            boxes[:, [0,2]] = w - boxes[:, [2,0]]\n",
    "            target['boxes'] = boxes\n",
    "            # flip keypoints: x -> w - x if visible\n",
    "            kps = target['keypoints']\n",
    "            if kps is not None:\n",
    "                kps = kps.clone()\n",
    "                kps[:, :, 0] = w - kps[:, :, 0]\n",
    "                target['keypoints'] = kps\n",
    "        return image, target\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "        ToTensor(),\n",
    "        RandomHorizontalFlip(0.5),\n",
    "    ])\n",
    "\n",
    "def get_test_transforms():\n",
    "    return Compose([\n",
    "        ToTensor(),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1e8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "782c05e8",
   "metadata": {},
   "source": [
    "#### Model definition (Keypoint R-CNN)\n",
    "\n",
    "    Use torchvision's keypointrcnn_resnet50_fpn. Important: set num_keypoints=1 (you have a single plucking point). The num_classes is num_background + classes â†’ if only 1 foreground class then num_classes=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f781e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torchvision\n",
    "from torchvision.models.detection.keypoint_rcnn import KeypointRCNN\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn\n",
    "\n",
    "def get_model(num_classes=2, num_keypoints=1, pretrained_backbone=True):\n",
    "    # load model with pre-trained weights on COCO backbone\n",
    "    model = keypointrcnn_resnet50_fpn(pretrained=True, progress=True,\n",
    "                                      num_classes=num_classes,\n",
    "                                      pretrained_backbone=pretrained_backbone,\n",
    "                                      num_keypoints=num_keypoints)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba775987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40247e7c",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "   A typical training loop adapted from torchvision references. Save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70091456",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset_saffron'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, random_split\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataset_saffron\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SaffronKeypointDataset\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_train_transforms, get_test_transforms\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dataset_saffron'"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from dataset_saffron import SaffronKeypointDataset\n",
    "from transforms import get_train_transforms, get_test_transforms\n",
    "from model import get_model\n",
    "import utils  # helper functions from torchvision references or simple collate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50):\n",
    "    model.train()\n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print(f\"Epoch {epoch} Iter {i} Loss {losses.item():.4f}\")\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    images_dir = \"Dataset_txt/image\"\n",
    "    ann_json = \"annotations_coco_like.json\"\n",
    "\n",
    "    dataset_full = SaffronKeypointDataset(images_dir, ann_json, transforms=get_train_transforms())\n",
    "    # split\n",
    "    n = len(dataset_full)\n",
    "    val_size = int(n*0.2)\n",
    "    train_size = n - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset_full, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "    model = get_model(num_classes=2, num_keypoints=1)\n",
    "    model.to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=20)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # save checkpoint\n",
    "        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pth\")\n",
    "\n",
    "    # Save final\n",
    "    torch.save(model.state_dict(), \"keypointrcnn_saffron_final.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735ceef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = SaffronKeypointDataset(\"Dataset_txt/image\", \"annotations_coco_like.json\")\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = get_model().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD([p for p in model.parameters() if p.requires_grad],\n",
    "                            lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for images, targets in data_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb58b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a84bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f366cfa3",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "    For full detection+keypoint evaluation use COCO keypoints metrics (AP for detection and keypoint AP). Pycocotools and proper annotation format are required. Alternatively compute simpler metrics:\n",
    "\n",
    "    Detection: mAP via torchvision.ops.boxes / external evaluator.\n",
    "\n",
    "    Keypoint correctness: PCK (Percentage of Correct Keypoints) â€” consider a keypoint correct if distance between predicted and GT is <= alpha * max(box_width, box_height) (alpha e.g. 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b535d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_utils.py\n",
    "import numpy as np\n",
    "\n",
    "def pck_single(gt_kp, pred_kp, bbox, alpha=0.1):\n",
    "    # gt_kp, pred_kp: [x,y,v]\n",
    "    if gt_kp[2] == 0:\n",
    "        return None  # not annotated\n",
    "    gt = np.array(gt_kp[:2])\n",
    "    pred = np.array(pred_kp[:2])\n",
    "    w = bbox[2] - bbox[0]\n",
    "    h = bbox[3] - bbox[1]\n",
    "    thresh = alpha * max(w, h)\n",
    "    dist = np.linalg.norm(pred - gt)\n",
    "    return dist <= thresh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016b86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "949f8299",
   "metadata": {},
   "source": [
    "#### Inference & visualization\n",
    "\n",
    "    Load model, run on an image, filter by score, and draw box + keypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07eb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference.py\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms.functional as F\n",
    "from model import get_model\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = get_model(num_classes=2, num_keypoints=1)\n",
    "model.load_state_dict(torch.load(\"keypointrcnn_saffron_final.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_and_visualize(image_path, score_thresh=0.6, save_path=None):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model([img_tensor])\n",
    "    out = outputs[0]\n",
    "    boxes = out['boxes'].cpu()\n",
    "    scores = out['scores'].cpu()\n",
    "    keypoints = out['keypoints'].cpu()  # shape (num_dets, num_kpts, 3)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    for i, s in enumerate(scores):\n",
    "        if s < score_thresh:\n",
    "            continue\n",
    "        box = boxes[i].numpy().tolist()\n",
    "        draw.rectangle(box, outline=\"red\", width=2)\n",
    "        kpt = keypoints[i][0]  # only one keypoint\n",
    "        x, y, v = float(kpt[0]), float(kpt[1]), float(kpt[2])\n",
    "        if v > 0:\n",
    "            r = 3\n",
    "            draw.ellipse((x-r, y-r, x+r, y+r), fill=\"blue\")\n",
    "\n",
    "    if save_path:\n",
    "        img.save(save_path)\n",
    "    return img\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    img = predict_and_visualize(\"images/example.jpg\", save_path=\"out_example.jpg\")\n",
    "    img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64eeb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov11_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
